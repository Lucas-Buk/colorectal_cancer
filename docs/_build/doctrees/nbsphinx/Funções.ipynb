{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Importações**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-24T18:13:40.814616Z",
     "iopub.status.busy": "2023-01-24T18:13:40.814616Z",
     "iopub.status.idle": "2023-01-24T18:13:40.848743Z",
     "shell.execute_reply": "2023-01-24T18:13:40.846257Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install shap\n",
    "# !pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-24T18:13:40.850933Z",
     "iopub.status.busy": "2023-01-24T18:13:40.850933Z",
     "iopub.status.idle": "2023-01-24T18:13:45.093125Z",
     "shell.execute_reply": "2023-01-24T18:13:45.093125Z"
    },
    "id": "CGkIAQ6fccMT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucas\\anaconda3\\envs\\colorectalEnv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "seed = 10 # semente para o random_state\n",
    "\n",
    "# Básicas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Modelos\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import RandomSampler\n",
    "from optuna.visualization import plot_optimization_history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Funções**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-24T18:13:45.093125Z",
     "iopub.status.busy": "2023-01-24T18:13:45.093125Z",
     "iopub.status.idle": "2023-01-24T18:13:46.276868Z",
     "shell.execute_reply": "2023-01-24T18:13:46.276868Z"
    },
    "id": "a9rewo-Orwqy"
   },
   "outputs": [],
   "source": [
    "# Graphs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, MaxAbsScaler, QuantileTransformer, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn import tree\n",
    "\n",
    "# SHAP values\n",
    "import shap\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "def read_csv(path, drop_id=False):\n",
    "    \"\"\"\"Read csv files\n",
    "\n",
    "    :param path str: path to the csv file.\n",
    "\n",
    "    :return: dataframe from the csv file.\n",
    "    :rtype: pd.DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(path, \n",
    "                    #    dtype={'M': str}\n",
    "                    )\n",
    "    if drop_id:\n",
    "        df.drop(columns=['ID_estudo'], inplace=True)\n",
    "\n",
    "    print(df.shape)\n",
    "\n",
    "    return df\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "def save_csv(df, path):\n",
    "    \"\"\"Save csv files\n",
    "\n",
    "    :param df pd.DataFrame: dataframe to be saved.\n",
    "    :param path str: path to save the csv file.\n",
    "\n",
    "    :return: no value\n",
    "    :rtype: none\n",
    "    \"\"\"\n",
    "\n",
    "    df.to_csv(path, encoding='utf-8', index=False)\n",
    "    print('CSV file saved successfully!')\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "def variables_preprocessing(df):\n",
    "    \"\"\"Do some preprocessing on the DataFrame like strings splits, fill NaN values,\n",
    "       replace values and drop some columns.\n",
    "\n",
    "    :param df pd.DataFrame: DataFrame to be preprocessed.\n",
    "\n",
    "    :return: DataFrame after be preprocessed and get some columns removed\n",
    "    :rtype: pd.DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    df_aux = df.copy()\n",
    "    no_info = '**Sem informação**'\n",
    "\n",
    "    # Excluding ECGRUP with X and Y values\n",
    "    df_aux = df_aux[~df_aux.ECGRUP.isin(['X','Y'])]\n",
    "\n",
    "    # Get 'comportamento' = 3\n",
    "    df_aux = df_aux[df_aux.comportamento == 3]\n",
    "\n",
    "    # Select only morphologies 81403\n",
    "    df_aux = df_aux[df_aux.MORFO == 81403]\n",
    "\n",
    "    # DRS\n",
    "    DRS_expand = df_aux.DRS.str.split(' ', expand=True)\n",
    "    df_aux['DRS'] = DRS_expand[1]\n",
    "    df_aux.DRS.fillna(0, inplace=True)\n",
    "\n",
    "    # META\n",
    "    # df_aux.META01.fillna(no_info, inplace=True)\n",
    "    # df_aux.META02.fillna(no_info, inplace=True)\n",
    "    # df_aux.META03.fillna(no_info, inplace=True)\n",
    "    # df_aux.META04.fillna(no_info, inplace=True)\n",
    "\n",
    "    # REC\n",
    "    # df_aux.REC01.fillna(no_info, inplace=True)\n",
    "    # df_aux.REC02.fillna(no_info, inplace=True)\n",
    "    # df_aux.REC03.fillna(no_info, inplace=True)\n",
    "\n",
    "    df_sp = df_aux[df_aux.UFRESID == 'SP']\n",
    "\n",
    "    col = df_sp.columns\n",
    "    drop_cols = ['UFRESID', 'UFNASC', 'REC04', 'CIDADE', 'DESCTOPO', 'DESCMORFO',\n",
    "                 'META01', 'META02', 'META03', 'META04', 'REC01', 'REC02', 'REC03',\n",
    "                 'comportamento', 'MORFO', 'TMOAPOS', 'TOPO', 'TOPOGRUP', 'T', \n",
    "                 'N', 'M', 'NAOTRAT', 'TRATAMENTO', 'TRATFAPOS', 'NENHUMAPOS', \n",
    "                 'CIRURAPOS', 'RADIOAPOS', 'QUIMIOAPOS', 'HORMOAPOS', 'IMUNOAPOS',\n",
    "                 'OUTROAPOS', 'RECLOCAL', 'RECREGIO', 'RECDIST', 'HABILIT']\n",
    "\n",
    "    col = col.drop(drop_cols)\n",
    "\n",
    "    return df_sp[col]\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "def get_dates_diff(df, dates_list):\n",
    "    \"\"\"Get the difference, in days, between columns with dates\n",
    "\n",
    "    :param df pd.DataFrame: DataFrame to get the dates difference.\n",
    "    :param dates_list list: list with the name of date columns.\n",
    "\n",
    "    :return: DataFrame with dates difference in nine new columns \n",
    "    :rtype: pd.DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    df_aux = df.copy()\n",
    "    \n",
    "    df_aux.dropna(subset=['DTTRAT', 'DTULTINFO'], inplace=True)\n",
    "    \n",
    "    for c in dates_list:\n",
    "        if c in ['DTTRAT', 'DTULTINFO', 'DTRECIDIVA']: # Has a different date format \n",
    "            fmt = '%Y-%m-%d %H:%M:%S'\n",
    "        else:\n",
    "            fmt = '%Y-%m-%d'\n",
    "        df_aux[c] = pd.to_datetime(df_aux[c], format=fmt)\n",
    "        \n",
    "    df_aux['CONSDIAG'] = (df_aux.DTDIAG - df_aux.DTCONSULT).dt.days\n",
    "    df_aux['DIAGTRAT'] = (df_aux.DTTRAT - df_aux.DTDIAG).dt.days\n",
    "    df_aux['TRATCONS'] = (df_aux.DTTRAT - df_aux.DTCONSULT).dt.days\n",
    "\n",
    "    # df_aux['RECCONS'] = (df_aux.DTRECIDIVA - df_aux.DTCONSULT).dt.days\n",
    "    # df_aux['RECDIAG'] = (df_aux.DTRECIDIVA - df_aux.DTDIAG).dt.days\n",
    "    # df_aux['RECTRAT'] = (df_aux.DTRECIDIVA - df_aux.DTTRAT).dt.days\n",
    "\n",
    "    df_aux['ULTICONS'] = (df_aux.DTULTINFO - df_aux.DTCONSULT).dt.days\n",
    "    df_aux['ULTIDIAG'] = (df_aux.DTULTINFO - df_aux.DTDIAG).dt.days\n",
    "    df_aux['ULTITRAT'] = (df_aux.DTULTINFO - df_aux.DTTRAT).dt.days\n",
    "\n",
    "    df_aux.drop(columns=['DTCONSULT', 'DTDIAG', 'DTTRAT', 'DTRECIDIVA', 'DTULTINFO'],\n",
    "                inplace=True)\n",
    "\n",
    "    return df_aux\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "def get_labels(df):\n",
    "    \"\"\"Create death labels acording to the last information year.\n",
    "\n",
    "    :param df pd.DataFrame: dataframe to be processed.\n",
    "\n",
    "    :return: DataFrame with the new labels\n",
    "    :rtype: pd.DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    df_aux = df.copy()\n",
    "\n",
    "    df_aux['obito_geral'] = 0\n",
    "    df_aux['obito_cancer'] = 0\n",
    "\n",
    "    df_aux['vivo_ano1'] = 0\n",
    "    df_aux['vivo_ano3'] = 0\n",
    "    df_aux['vivo_ano5'] = 0 \n",
    "    \n",
    "    df_aux.loc[df_aux.ULTINFO > 2, 'obito_geral'] = 1\n",
    "\n",
    "    df_aux.loc[df_aux.ULTINFO == 3, 'obito_cancer'] = 1\n",
    "\n",
    "    df_aux.loc[df_aux.ULTIDIAG > 365, 'vivo_ano1'] = 1\n",
    "    df_aux.loc[df_aux.ULTIDIAG > 3*365, 'vivo_ano3'] = 1\n",
    "    df_aux.loc[df_aux.ULTIDIAG > 5*365, 'vivo_ano5'] = 1\n",
    "\n",
    "    return df_aux\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "def get_label_rec(df):\n",
    "    \"\"\"Create the labels analyzing whether there was recurrence.\n",
    "    \n",
    "    :param df pd.DataFrame: dataframe to be processed.\n",
    "\n",
    "    :return: DataFrame with the new labels\n",
    "    :rtype: pd.DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    df_aux = df.copy()\n",
    "\n",
    "    df_aux['ob_com_rec'] = 0\n",
    "    df_aux['ob_sem_rec'] = 0\n",
    "    df_aux['vivo_com_rec'] = 0\n",
    "    df_aux['vivo_sem_rec'] = 0\n",
    "\n",
    "    df_aux.loc[(df_aux.obito_geral == 1) & (df_aux.RECNENHUM == 1), 'ob_sem_rec'] = 1\n",
    "    df_aux.loc[(df_aux.obito_geral == 1) & (df_aux.RECNENHUM == 0), 'ob_com_rec'] = 1\n",
    "    df_aux.loc[(df_aux.obito_geral == 0) & (df_aux.RECNENHUM == 1), 'vivo_sem_rec'] = 1\n",
    "    df_aux.loc[(df_aux.obito_geral == 0) & (df_aux.RECNENHUM == 0), 'vivo_com_rec'] = 1\n",
    "\n",
    "    return df_aux\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "def get_train_test(df, drop_cols, label, test_size=0.25, random_state=0):\n",
    "    \"\"\"Get features and label, and then returns train and test dataframes.\n",
    "\n",
    "    :param df pd.DataFrame: dataframe that will be splitted.\n",
    "    :param drop_cols list: columns to be removed from the DataFrame.\n",
    "    :param label str: name of the label column.\n",
    "    :param test_size float: size of test (default=0.25).\n",
    "    :param random_state int: value for train_test_split random_state (default=10).\n",
    "\n",
    "    :return: train and test DataFrames, X_train, X_test, y_train, y_test\n",
    "    :rtype: pd.DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    df_aux = df.copy()\n",
    "\n",
    "    cols = df_aux.columns.drop(drop_cols)\n",
    "    lb = df_aux[label].copy()\n",
    "    cols = cols.drop(label)\n",
    "    feat = df_aux[cols]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(feat, lb, \n",
    "                                                        test_size=test_size, \n",
    "                                                        random_state=random_state,\n",
    "                                                        stratify=lb)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "def train_preprocessing(df, encoder_type='LabelEncoder', normalizer='StandardScaler',\n",
    "                        pca=False, pca_components=None, random_state=0):\n",
    "    \"\"\"Preprocessing the train dataset.\n",
    "\n",
    "    :param df pd.DataFrame: DataFrame to be preprocessed.\n",
    "    :param encoder_type string: Encoder type to use for categorical features (default='LabelEncoder').\n",
    "        options:\n",
    "        * 'LabelEncoder'\n",
    "        * 'OneHotEncoder'\n",
    "    :param normalizer str: which normalizer to be fitted to the data (default='StandardScaler').\n",
    "        options:\n",
    "        * 'StandardScaler'\n",
    "        * 'MinMaxScaler'\n",
    "        * 'MaxAbsScaler'\n",
    "        * 'QuantileTransformer'\n",
    "    :param pca bool: if want to use PCA components set True (default=False).\n",
    "    :param pca_components int: number of PCA components (default=None).\n",
    "    :param random_state int: value for pca random_state (default=10).\n",
    "\n",
    "    :return df: preprocessed train DataFrame \n",
    "    :rtype: pd.DataFrame\n",
    "    :return enc: trained LabelEncoder \n",
    "    :rtype: dict\n",
    "    :return norm: trained normalizer \n",
    "    :rtype: object\n",
    "    :return pca if param pca=True: trained PCA \n",
    "    :rtype: object\n",
    "    :return feat_cols: list with features names\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "\n",
    "    df_aux = df.copy()\n",
    "\n",
    "    list_categorical = df_aux.select_dtypes(include='object').columns\n",
    "\n",
    "    enc = dict()\n",
    "    if encoder_type == 'LabelEncoder':\n",
    "        for col in list_categorical:\n",
    "            enc[col] = LabelEncoder()\n",
    "            df_aux[col] = enc[col].fit_transform(df_aux[col])\n",
    "\n",
    "    elif encoder_type == 'OneHotEncoder':\n",
    "        for col in list_categorical:\n",
    "            enc[col] = OneHotEncoder(handle_unknown='ignore')\n",
    "            ohe_results = enc[col].fit_transform(df_aux[[col]])\n",
    "            df1 = pd.DataFrame(ohe_results.toarray(), columns=[f'{col}_{name}' for name in enc[col].categories_[0]], index=df_aux[col].index)\n",
    "            df_aux = df_aux.merge(df1, how='left', left_index=True, right_index=True)\n",
    "\n",
    "        df_aux.drop(columns=list_categorical, inplace=True)\n",
    "\n",
    "    feat_cols = df_aux.columns\n",
    "\n",
    "    if normalizer == 'StandardScaler':\n",
    "        norm = StandardScaler()\n",
    "    elif normalizer == 'MinMaxScaler':\n",
    "        norm = MinMaxScaler((0, 1))\n",
    "    elif normalizer == 'MaxAbsScaler':\n",
    "        norm = MaxAbsScaler()\n",
    "    elif normalizer == 'QuantileTransformer':\n",
    "        norm = QuantileTransformer(output_distribution='normal')\n",
    "    \n",
    "    df_aux = norm.fit_transform(df_aux)\n",
    "\n",
    "    if pca:\n",
    "        pca = PCA(pca_components, random_state=random_state)\n",
    "        df_aux = pca.fit_transform(df_aux)\n",
    "\n",
    "        return df_aux, enc, norm, pca, feat_cols\n",
    "\n",
    "    else:\n",
    "        return df_aux, enc, norm, feat_cols\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "def test_preprocessing(df, enc, norm, encoder_type='LabelEncoder', pca=None):\n",
    "    \"\"\"Preprocessing the test dataset.\n",
    "\n",
    "    :param df pd.DataFrame: DataFrame to be preprocessed.\n",
    "    :param enc: trained encoder with the categorical features.\n",
    "    :param norm: trained normalizer.\n",
    "    :param encoder_type string: Encoder type to use for categorical features (default='LabelEncoder').\n",
    "        options:\n",
    "        * 'LabelEncoder'\n",
    "        * 'OneHotEncoder'\n",
    "    :param pca: trained PCA (default=None).\n",
    "\n",
    "    :return: preprocessed test DataFrame \n",
    "    :rtype: pd.DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    df_aux = df.copy()\n",
    "\n",
    "    df_aux.fillna(0, inplace=True)\n",
    "\n",
    "    list_categorical = df_aux.select_dtypes(include='object').columns\n",
    "\n",
    "    if encoder_type == 'LabelEncoder':\n",
    "        for col in list_categorical:\n",
    "            df_aux.loc[~df_aux[col].isin(enc[col].classes_), col] = -1 \n",
    "            df_aux.loc[df_aux[col].isin(enc[col].classes_), col] = enc[col].transform(df_aux[col][df_aux[col].isin(enc[col].classes_)])\n",
    "    \n",
    "    elif encoder_type == 'OneHotEncoder':\n",
    "        for col in list_categorical:\n",
    "            ohe_results = enc[col].transform(df_aux[[col]])\n",
    "            df1 = pd.DataFrame(ohe_results.toarray(), columns=[f'{col}_{name}' for name in enc[col].categories_[0]], index=df_aux[col].index)\n",
    "            df_aux = df_aux.merge(df1, how='left', left_index=True, right_index=True)\n",
    "\n",
    "        df_aux.drop(columns=list_categorical, inplace=True)\n",
    "\n",
    "    df_aux = norm.transform(df_aux)\n",
    "\n",
    "    if pca != None:\n",
    "        df_aux = pca.transform(df_aux)\n",
    "\n",
    "    return df_aux \n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "def preprocessing(df, cols_drop, label, test_size=0.25, encoder_type='LabelEncoder',\n",
    "                  norm_name='StandardScaler', return_enc_norm=False, pca=False, \n",
    "                  pca_components=None, balance_data=True, group_years=False,\n",
    "                  first_year=None, last_year=None, morpho3=False, random_state=0):\n",
    "  \n",
    "    \"\"\"Preprocessing the train and test datasets.\n",
    "\n",
    "    :param df pd.DataFrame: DataFrame to be preprocessed.\n",
    "    :param cols_drop list: list of columns to be dropped from dataset.\n",
    "    :param label string: name of the column that will be the label.\n",
    "    :param test_size float: size of test set (default=0.25).\n",
    "    :param encoder_type string: Encoder type to use for categorical features (default='LabelEncoder').\n",
    "        options:\n",
    "        * 'LabelEncoder'\n",
    "        * 'OneHotEncoder'\n",
    "    :param norm_name str: which normalizer to be fitted to the data (default='StandardScaler').\n",
    "        - options:\n",
    "        * 'StandardScaler';\n",
    "        * 'MinMaxScaler';\n",
    "        * 'MaxAbsScaler';\n",
    "        * 'PowerTransformer';\n",
    "        * 'QuantileTransformer'.\n",
    "    :param return_enc_norm bool: if want to return the encoder and the normalizer set True (default=False).\n",
    "    :param pca bool: if want to use PCA components set True (default=False).\n",
    "    :param pca_components int: number of PCA components (default=None).\n",
    "    :param balance_data bool: balance the data using oversampling (default=True).\n",
    "    :param group_years bool: create a subset with years grouped (default=False).\n",
    "    :param first_year int: first year of the grouped years. Ignored if group_years = False.\n",
    "    :param last_year int: last year of the grouped years. Ignored if group_years = False.\n",
    "    :param morpho3 bool: use only morphologies that the last number is equal to 3 (default=False).\n",
    "    :param random_state int: value for pca random_state (default=10).\n",
    "\n",
    "    :return X_train_: preprocessed train DataFrame \n",
    "    :rtype: pd.DataFrame\n",
    "    :return X_test_: preprocessed test DataFrame \n",
    "    :rtype: pd.DataFrame\n",
    "    :return y_train_: preprocessed train label \n",
    "    :rtype: pd.DataFrame\n",
    "    :return y_test: preprocessed test label \n",
    "    :rtype: pd.DataFrame\n",
    "    :return feat_cols: list with the features columns names\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "\n",
    "    df_aux = df.copy()\n",
    "\n",
    "    # Morphology 3\n",
    "    if morpho3:\n",
    "        df_aux['comportamento'] = [int(repr(i)[-1]) for i in df_aux.MORFO]\n",
    "        df_aux = df_aux[df_aux.comportamento == 3].copy()\n",
    "        df_aux.drop(columns='comportamento', inplace=True)\n",
    "\n",
    "    # Grouped years\n",
    "    if group_years and first_year != None and last_year != None:\n",
    "        df_aux = df_aux[(df_aux.ANODIAG >= first_year) & (df_aux.ANODIAG <= last_year)].copy()\n",
    "        \n",
    "    # Train Test split\n",
    "    X_train, X_test, y_train, y_test = get_train_test(df_aux, cols_drop, label, \n",
    "                                                      test_size, \n",
    "                                                      random_state=random_state)\n",
    "\n",
    "    # Preprocessing\n",
    "    if pca and pca_components != None:\n",
    "        X_train_enc, enc, norm, pca, feat_cols = train_preprocessing(X_train, encoder_type=encoder_type, \n",
    "                                                                     normalizer=norm_name, pca=pca,\n",
    "                                                                     pca_components=pca_components,\n",
    "                                                                     random_state=random_state)\n",
    "        X_test_ = test_preprocessing(X_test, enc, norm, \n",
    "                                     encoder_type, pca)\n",
    "\n",
    "    else:\n",
    "        X_train_enc, enc, norm, feat_cols = train_preprocessing(X_train, encoder_type=encoder_type,\n",
    "                                                                normalizer=norm_name)\n",
    "        X_test_ = test_preprocessing(X_test, enc, norm, encoder_type)\n",
    "\n",
    "    # Balancing\n",
    "    if balance_data:\n",
    "        X_train_, y_train_ = SMOTE(random_state=random_state).fit_resample(X_train_enc, y_train)\n",
    "    \n",
    "    else:\n",
    "        X_train_, y_train_ = X_train_enc, y_train\n",
    "\n",
    "    print(f'X_train = {X_train_.shape}, X_test = {X_test_.shape}')\n",
    "    print(f'y_train = {y_train_.shape}, y_test = {y_test.shape}')\n",
    "\n",
    "    if return_enc_norm:\n",
    "        return X_train_, X_test_, y_train_, y_test, feat_cols, enc, norm\n",
    "    else:\n",
    "        return X_train_, X_test_, y_train_, y_test, feat_cols\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "def show_tree(model, feat_cols, max_depth=3, estimator=0):\n",
    "    \"\"\"Show the Random Forest tree\n",
    "\n",
    "    :param model: machine learning model.\n",
    "    :param feat_cols list: list of the features used in the model training.\n",
    "    :param max_depth int: max_depth to show in the tree (default = 3).\n",
    "    :param estimator int: number of the estimator do show the tree (default = 0).\n",
    "\n",
    "    :return: no value\n",
    "    :rtype: none\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure(figsize = (22, 10))\n",
    "    tree.plot_tree(model.estimators_[estimator],\n",
    "                   feature_names=feat_cols,\n",
    "                   filled=True, \n",
    "                   max_depth=max_depth);\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "def plot_feat_importances(model, feat_cols, n=10):\n",
    "    \"\"\"Shows the features importances for the model.\n",
    "\n",
    "    :param model: machine learning model.\n",
    "    :param feat_cols list: list of the features used in the model training.\n",
    "    :param n int: number of features to be shown (default=10).\n",
    "\n",
    "    :return: no value\n",
    "    :rtype: none\n",
    "    \"\"\"\n",
    "\n",
    "    feat_import = pd.Series(model.feature_importances_, index=feat_cols)\n",
    "    feat_import.nlargest(n).plot(kind='barh', figsize=(10, 8))\n",
    "    plt.show()\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "def plot_roc_curve(model, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Plot the ROC curve for train and test sets.\n",
    "\n",
    "    :param model: Trained machine learning model.\n",
    "    :param X_train: Features of training set.\n",
    "    :param X_test: Features of test set.\n",
    "    :param y_train: Label of training set.\n",
    "    :param y_test: Label of test set.\n",
    "\n",
    "    :return: no value\n",
    "    :rtype: None\n",
    "    \"\"\"\n",
    "    probas_train = model.predict_proba(X_train)[:, 1]\n",
    "    probas_test = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    fp_train, tp_train, _ = roc_curve(y_train, probas_train)\n",
    "    fp_test, tp_test, _ = roc_curve(y_test, probas_test)\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(fp_train, tp_train, 'b', label=f'Train (AUC = {auc(fp_train, tp_train):.3f})')\n",
    "    plt.plot(fp_test, tp_test, 'r', label=f'Test (AUC = {auc(fp_test, tp_test):.3f})')\n",
    "    plt.plot(np.linspace(0, 1, 100),\n",
    "             np.linspace(0, 1, 100),\n",
    "             label='Baseline',\n",
    "             linestyle='--', \n",
    "             color='k')\n",
    "    plt.xlabel('False Positives')\n",
    "    plt.ylabel('True Positives')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "def plot_confusion_matrix(model, x, y, format='.3f'):\n",
    "    '''Plot the confusion matrix.\n",
    "\n",
    "    :param model: Trained machine learning model.\n",
    "    :param x: Features set.\n",
    "    :param y: Label set.\n",
    "    :param format string: Format for the numbers in the confusion matrix (default \".3f\")\n",
    "\n",
    "    :return: no value\n",
    "    :rtype: None\n",
    "    '''\n",
    "    with plt.rc_context({'font.size': 12, 'font.weight': 'bold'}):\n",
    "        ConfusionMatrixDisplay.from_estimator(model, x, y, values_format=format,\n",
    "                                              cmap='Blues', normalize='true')\n",
    "        plt.show()\n",
    "\n",
    "    print(f'\\n{classification_report(y, model.predict(x), digits=3)}')\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "def plot_shap_values(model, x, features, max_display=10):\n",
    "    \"\"\"Plot the shap values.\n",
    "\n",
    "    :param model: Trained machine learning model.\n",
    "    :param x: Features set.\n",
    "    :param features: Features names.\n",
    "    :param max_display int: Max features to show shap values (default=10)\n",
    "\n",
    "    :return: no value\n",
    "    :rtype: None\n",
    "    \"\"\"\n",
    "\n",
    "    shap_values = shap.TreeExplainer(model).shap_values(x)\n",
    "\n",
    "    try:\n",
    "        shap.summary_plot(shap_values[1], x, \n",
    "                          feature_names=features,\n",
    "                          max_display=max_display)\n",
    "    except AssertionError:\n",
    "        shap.summary_plot(shap_values, x, \n",
    "                          feature_names=features,\n",
    "                          max_display=max_display)\n",
    "        \n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "def roc_together(X, y, naive_bayes=None, random_forest=None, xgboost=None, \n",
    "                 lightgbm=None):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 7))\n",
    "\n",
    "    if naive_bayes != None:\n",
    "        probas_nb = naive_bayes.predict_proba(X)[:, 1]\n",
    "        fp_nb, tp_nb, _ = roc_curve(y, probas_nb)\n",
    "        plt.plot(fp_nb, tp_nb, 'k', linestyle='dashed',\n",
    "                 label=f'Naive Bayes (AUC = {auc(fp_nb, tp_nb):.3f})')\n",
    "        \n",
    "    if random_forest != None:\n",
    "        probas_rf = random_forest.predict_proba(X)[:, 1]\n",
    "        fp_rf, tp_rf, _ = roc_curve(y, probas_rf)\n",
    "        plt.plot(fp_rf, tp_rf, 'k', linestyle='dashdot',\n",
    "                 label=f'Random Forest (AUC = {auc(fp_rf, tp_rf):.3f})')\n",
    "    \n",
    "    if xgboost != None:\n",
    "        probas_xgb = xgboost.predict_proba(X)[:, 1]\n",
    "        fp_xgb, tp_xgb, _ = roc_curve(y, probas_xgb)\n",
    "        plt.plot(fp_xgb, tp_xgb, 'k', \n",
    "                 label=f'XGBoost (AUC = {auc(fp_xgb, tp_xgb):.3f})')\n",
    "    \n",
    "    if lightgbm != None:\n",
    "        probas_lgbm = lightgbm.predict_proba(X)[:, 1] \n",
    "        fp_lgbm, tp_lgbm, _ = roc_curve(y, probas_lgbm)\n",
    "        plt.plot(fp_lgbm, tp_lgbm, 'k', linestyle='dashed',\n",
    "                 label=f'LightGBM (AUC = {auc(fp_lgbm, tp_lgbm):.3f})')\n",
    "    \n",
    "    plt.plot(np.linspace(0, 1, 100),\n",
    "             np.linspace(0, 1, 100),\n",
    "             label='Baseline',\n",
    "             linestyle='dotted', \n",
    "             color='gray')\n",
    "    plt.xlabel('False Positives')\n",
    "    plt.ylabel('True Positives')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "665796ea3363072d3a6057ac2fdbe3c4fcb0d17a4b92295d9707f78e9c46c0af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
